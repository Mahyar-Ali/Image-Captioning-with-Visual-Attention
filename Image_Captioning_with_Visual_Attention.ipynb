{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Captioning with Visual Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IOnx3ZFRd8i",
        "colab_type": "text"
      },
      "source": [
        "The code is inspired from the article [tensorflow tutorials](https://www.tensorflow.org/tutorials/text/image_captioning)\n",
        ".My work was only the understanding of code and changing the model architecture a bit in order to create a replica. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oI_7cwrK4ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IMporting the required Dependencies\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61c6ZSmRu5Hn",
        "colab_type": "text"
      },
      "source": [
        "# Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tecPIHX1MHiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-Z1mTnDLJPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip annotations_trainval2014.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMR7z6q8L1Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotation_file = \"/content/gdrive/My Drive/image_captioning/annotations/captions_train2014.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV6fKA4UMM_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://images.cocodataset.org/zips/train2014.zip\n",
        "!unzip train2014.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrauzVb9MkXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_folder = './train2014/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhIC128Ou-9M",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYbFqgGYM680",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(annotation_file,'r') as f:\n",
        "  annotations = json.load(f)\n",
        "\n",
        "#Storing captions and Image names in vectors\n",
        "\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "  caption = '<start> '+annot['caption']+' <end>'\n",
        "  image_id = annot['image_id']\n",
        "  full_coco_image_path = image_folder+\"COCO_train2014_\"+'%012d.jpg' % (image_id)\n",
        "  all_img_name_vector.append(full_coco_image_path)\n",
        "  all_captions.append(caption)\n",
        "\n",
        "#Shuffling captions anf image names\n",
        "\n",
        "train_captions,img_name_vectors = shuffle(all_captions,all_img_name_vector,\n",
        "                                            random_state = 1)\n",
        "\n",
        "train_captions = train_captions[60000:90000]\n",
        "img_name_vectors = img_name_vectors[60000:90000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASH2Wu3VThUq",
        "colab_type": "code",
        "outputId": "cf875413-b684-4922-a944-bed332680fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(len(train_captions),len(all_captions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30000 414113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5_GF5n6VSQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(file_path):\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = tf.image.decode_jpeg(img,channels=3)\n",
        "  img = tf.image.resize(img,(299,299))\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  return img,file_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuSY_GTvZ6Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input,hidden_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGy2zA2OabSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get unique Images\n",
        "encode_train = sorted(set(img_name_vectors))\n",
        "\n",
        "#Creating the Bottle Neck Features and saving it using pickle\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(load_image,num_parallel_calls = \n",
        "                                  tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img,path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0],-1,batch_features.shape[3]))\n",
        "  for bf,p in zip(batch_features,path):\n",
        "      path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "      np.save(path_of_feature,bf.numpy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX6tKwZFvzag",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzIYe_a1iWld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_max_length(tensor):\n",
        "  return max(len(t) for t in tensor )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwp2Y4XIkNEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k = 6000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token='<unk>',\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2FkyqkNmpUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs,padding='post')\n",
        "# Calculates the max_length, which is used to store the attention weights\n",
        "max_length = calc_max_length(train_seqs)\n",
        "\n",
        "#Splitting the data into training and testing\n",
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vectors,cap_vector,\n",
        "                                                                    test_size=0.001,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47Jv0m4n0Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE=1000\n",
        "embedding_dim = 256\n",
        "units=512\n",
        "vocab_size = top_k+1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "'''Shape of the vector extracted from InceptionV3 is (64, 2048)'''\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlaemdzcoYA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the numpy files\n",
        "def map_func(image_name,cap):\n",
        "  img_tensor = np.load(image_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor,cap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2yfiW54wHS9",
        "colab_type": "text"
      },
      "source": [
        "## Creating Tensorflow Dataset Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVAfz9zlos2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train,cap_train))\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "                      map_func,[item1,item2],[tf.float32,tf.int32]),              \n",
        "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoIra6f1psQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b = next(iter(dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y978r6qhwcXN",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9djjqT7etNbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self,units):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V1 = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self,features,hidden):\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden,1)\n",
        "    score = tf.nn.tanh(self.W1(hidden_with_time_axis)+\n",
        "                       self.W2(features))\n",
        "    attention_weights = tf.nn.softmax(self.V1(score),axis=1)\n",
        "\n",
        "\n",
        "    context_vector = attention_weights*features\n",
        "    context_vector = tf.reduce_sum(context_vector,axis=1)\n",
        "    return context_vector,attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrcAe_Pkvd-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "  def __init__(self,embedding_dim):\n",
        "    super(CNN_Encoder,self).__init__()\n",
        "    self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.fc(x)\n",
        "    x = tf.nn.tanh(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSiRecbkxzYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self,embedding_dim,units,vocab_size):\n",
        "    super(RNN_Decoder,self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
        "    self.GRU = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "    self.drop1 = tf.keras.layers.Dropout(0.35)\n",
        "    self.batch_norm = tf.keras.layers.BatchNormalization()\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self,x,features,hidden):\n",
        "    context_vector,attention_weights = self.attention(features,hidden)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    x = tf.concat([tf.expand_dims(context_vector,1),x],axis=-1)\n",
        "\n",
        "    output,state = self.GRU(x)\n",
        "    x = self.fc1(output)\n",
        "    x = self.drop1(x)\n",
        "    x = self.batch_norm(x)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x,state,attention_weights\n",
        "\n",
        "  def reset_state(self,batch_sz):\n",
        "    return tf.zeros([batch_sz,self.units])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu3j5J6Iv9qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim,units,vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqCh4IjLsjV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Just in order to save attention weights\n",
        "attention = BahdanauAttention(units)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRBdIKo48IPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loss and Optimizer\n",
        "optimizer=tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
        "\n",
        "def loss_function(real,pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMbGJni2wsJP",
        "colab_type": "text"
      },
      "source": [
        "To restore the saved weights we hve to initialize the custom Models first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d64CAIK4tM2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_x = encoder(np.ones([1,64,2048]))\n",
        "encoder.load_weights(\"/content/gdrive/My Drive/image_captioning/weights1.hdf5\")\n",
        "c = decoder(tf.expand_dims([tokenizer.word_index['<start>']]*BATCH_SIZE,1),test_x,decoder.reset_state(BATCH_SIZE))\n",
        "decoder.load_weights(\"/content/gdrive/My Drive/image_captioning/weights2.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3efqjSPMCKFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#In case we need to store the model as checkpoint\n",
        "ckpt_path = \"/content/gdrive/My Drive/model_checkpoint_2/\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt,ckpt_path,max_to_keep=3)\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  if ckpt_manager.latest_checkpoint:\n",
        "    print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
        "  else:\n",
        "    print(\"Initializing from scratch.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Dyqvw4Enxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To plot attention\n",
        "loss_plot = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNEsweTBxEMo",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M68RS-kXE5_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(img_tensor,target):\n",
        "  loss = 0\n",
        "  hidden = decoder.reset_state(batch_sz=target.shape[0])\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']]*target.shape[0],1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    features = encoder(img_tensor)\n",
        "    for i in range(target.shape[1]):\n",
        "      prediction,hidden,_ = decoder(dec_input,features,hidden)\n",
        "      loss += loss_function(target[:,i],prediction)\n",
        "      dec_input = tf.expand_dims(target[:,i],1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "  trainable_variables = encoder.trainable_variables+decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss,trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,trainable_variables))\n",
        "  return loss, total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsELG9dCGsTL",
        "colab_type": "code",
        "outputId": "927ef198-babe-47ca-ef27-2fc014ad5b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "EPOCHS = 30\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  total_loss = 0\n",
        "  for (batch,(img_tensor,target)) in enumerate(dataset):\n",
        "    batch_loss, t_loss = train_step(img_tensor,target)\n",
        "    total_loss += t_loss\n",
        "    if batch%100 == 0:\n",
        "        print(\"Epoch:{}.....Batch:{}.....Loss:{}\".format(epoch+1,\n",
        "                                                         batch,batch_loss.numpy() / int(target.shape[1])))\n",
        "    \n",
        "    loss_plot.append(total_loss/num_steps)\n",
        "    loss_plot_2.append(total_loss)\n",
        "  if epoch%1==0:\n",
        "      ckpt_manager.save()\n",
        "      encoder.save_weights(\"/content/gdrive/My Drive/image_captioning/weights1.hdf5\",overwrite=True)\n",
        "      attention.save_weights(\"/content/gdrive/My Drive/image_captioning/weights3.hdf5\",overwrite=True)\n",
        "      decoder.save_weights(\"/content/gdrive/My Drive/image_captioning/weights2.hdf5\",overwrite=True)\n",
        "  print('Epcoh:{}...................Loss:{}'.format(epoch+1,total_loss/num_steps))\n",
        "  print(\"Time Taken __ {:.3f} sec\".format(time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1.....Batch:0.....Loss:2.997594305809508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYFwouSKxKQm",
        "colab_type": "text"
      },
      "source": [
        "#Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwvSmVFZNDXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(image):\n",
        "  attention_plot = np.zeros([max_length,attention_features_shape])\n",
        "  hidden = decoder.reset_state(1)\n",
        "  temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "  img_tensor_val = image_features_extract_model(temp_input)\n",
        "  img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "  features = encoder(img_tensor_val)\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']],0)\n",
        "  result = []\n",
        "  for i in range(max_length):\n",
        "    prediction,hidden,attention_weights = decoder(dec_input,features,hidden)\n",
        "    attention_plot[i] = tf.reshape(attention_weights,(-1,)).numpy()\n",
        "    predicted_id = tf.random.categorical(prediction,1)[0][0].numpy()\n",
        "    result.append(tokenizer.index_word[predicted_id])\n",
        "    if tokenizer.index_word[predicted_id] == '<end>':\n",
        "        return result, attention_plot\n",
        "    dec_input = tf.expand_dims([predicted_id],0)\n",
        "  \n",
        "  attention_plot = attention_plot[:len(result),:]\n",
        "  return result,attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilzXpACTREOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention(image,result,attention_plot):\n",
        "  temp_image = np.array(Image.open(image))\n",
        "  fig = plt.figure(figsize=(15,15))\n",
        "  len_result = len(result)\n",
        "  for i in range(len_result):\n",
        "    temp_att = np.resize(attention_plot[i],(8,8))\n",
        "    ax =fig.add_subplot(len_result//2, len_result//2, i+1)\n",
        "    ax.set_title(result[i])\n",
        "    img = ax.imshow(temp_image)\n",
        "    ax.imshow(temp_att,cmap='gray',alpha=0.6,extent=img.get_extent())\n",
        "\n",
        "  #plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AhF20h8SxOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rid = np.random.randint(0,len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
        "result,attention_plot = evaluate(image)\n",
        "\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image, result, attention_plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRgcx8V6tdqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(np.array(Image.open(img_name_val[rid])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FysoeMrsUSsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_url = \"/content/test.jpeg\"\n",
        "result, attention_plot = evaluate(image_url)\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image_url, result, attention_plot)\n",
        "# opening the image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8JUJEQuvcKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}